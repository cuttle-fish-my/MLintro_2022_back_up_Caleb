\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{tikz}  
\usepackage{optidef}
\usepackage{pdfpages}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%  

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\rhead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{2em}

%
% Create Problem Sections
%


% \setcounter{secnumdepth}{0}
% \newcounter{partCounter}
% \newcounter{homeworkProblemCounter}
% \setcounter{homeworkProblemCounter}{1}


\newcommand{\hmwkTitle}{Homework V}
\newcommand{\hmwkDueDate}{Dec 20th, 2022}
\newcommand{\hmwkClass}{Introduction to Machine Learing}
\newcommand{\hmwkClassInstructor}{Professor Ziping Zhao}
\newcommand{\hmwkAuthorName}{Bingnan Li}
\newcommand{\hmwkAuthorID}{2020533092}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 11:59pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}\\ \hmwkAuthorID}
\date{}

% \renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\b}[1]{\bm{#1}}
\newcommand{\PARTIAL}[2]{\frac{\partial #1}{\partial #2}}
\begin{document}

\maketitle
\pagebreak

\begin{enumerate}
	\setlength\parindent{2em}
    \item [1.] [{Deep Learning Models}]
    \begin{enumerate}
	    \setlength\parindent{2em}
        \item [(a)] Consider a 3D convolution layer. Suppose the input size is $32 \times 32 \times 3$ (width, height, depth) and we use ten $5 \times 5$(width, height) kernels to convolve with it. Set stride = 1 and pad = 2. What is the output size? Let the bias for each kernel be a scalar, how many parameters do we have in this layer?\newline
        {\bf Solution:}
        \par The input data shape is \[[C_{in},D_{in},H_{in},W_{in}] = [1, 3, 32, 32]\] and the kernel size is \[[C_{out}, D_{kernel}, H_{kernel}, w_{kernel}] = [10, 3, 5, 5]\]
        Then by the formula, we have output data with a shape
        \[[C_{out}, D_{out}, H_{out}, W_{out}]\]
        and 
        \begin{align*}
                D_{out} &= \frac{D_{in}+2\times pad-(D_{kernel}-1)-1}{stride}+1=5\\
                H_{out} &= \frac{H_{in}+2\times pad-(H_{kernel}-1)-1}{stride}+1=32\\
                W_{out} &= \frac{W_{in}+2\times pad-(W_{kernel}-1)-1}{stride}+1=32\\
        \end{align*}
        \par Thus, the output size is $[C_{out}, D_{out}, H_{out}, W_{out}]=[10,5,32,32]$
        \par Moreover, the total number of parameters is 
        \[\#\{parameters\}=\#\{parameters\ in\ kernel\}+\#\{biases\} = 5\times 5\times 3\times 10 + 10=760\]
        \item [(b)] The convolution layer is followed by a max pooling layer with $2 \times 2$ (width, height) filter and stride = 2. What is the output size of the pooling layer? How many parameters do we have in the pooling layer?\newline
        {\bf Solution:}
        \par The input data shape is 
        \[[C_{in}, D_{in}, H_{in}, W_{in}]=[10,5,32,32]\]
        and the kernel shape is 
        \[[C_{out}, D_{kernel}, H_{kernel}, W_{kernel}] = [10,5,2,2]\]
        Then by the formula, we have output data with a shape
        \[[C_{out}, D_{out}, H_{out}, W_{out}]\]
        and 
        \begin{align*}
                D_{out} &= \frac{D_{in}+2\times pad-(D_{kernel}-1)-1}{stride}+1=1\\
                H_{out} &= \frac{H_{in}+2\times pad-(H_{kernel}-1)-1}{stride}+1=16\\
                W_{out} &= \frac{W_{in}+2\times pad-(W_{kernel}-1)-1}{stride}+1=16\\
        \end{align*}
        \par Thus, the output size is $[C_{out}, D_{out}, H_{out}, W_{out}]=[10,1,16,16]$.
        \par Moreover, given that the max pooling layer only performs maximizing, there are no parameters in the pooling layer. Thus, the total number of parameters of the pooling layer is $0$.
    \end{enumerate}
    \item [2.] [Deep Learning Models]
    \begin{enumerate}
        \item [(a)]
        \begin{enumerate}
	        \setlength\parindent{2em}
            \item [(i.)] When $r = 1$, $\b{W}$ is exactly the eigenvector of $\b{XX}^T$ corresponding to its largest eigenvalue.\newline
            {\bf Proof:}
            \par When $r = 1$, we know that $\b{W}^T\b{XX}^T\b{W}$ is a scalar, which means 
            \[Tr(\b{W}^T\b{XX}^T\b{W}) = \b{W}^T\b{XX}^T\b{W}\]
            Hence, the optimization problem can be rewritten as follows:
            \begin{maxi*}|l|
                {\b{W}}{\b{W}^T\b{XX}^T\b{W}}{}{}\quad s.t.\ \b{W}^T\b{W} = 1
            \end{maxi*}
            Then, the Lagrangian is:
            \begin{align*}
                \mathcal{L}(\b{W},\lambda) = \b{W}^T\b{XX}^T\b{W} - \lambda \b{W}^T\b{W}
            \end{align*}
            Taking the derivate of $\mathcal{L}$ w.r.t $\b{W}$ and setting it to $0$, we get
            \begin{align*}
                2\b{XX}^T\b{W}^\star-2\lambda^\star\b{W}^\star &= 0\\
                &\Rightarrow\\
                \b{XX}^T\b{W}^\star &= \lambda^\star\b{W}^\star
            \end{align*} 
            Thus, $\lambda^\star$ is eigenvalue of $\b{XX}^T$ and $\b{W}^\star$ is the corresponding eigenvector. Moreover, since
            \begin{align*}
                {\b{W^\star}}^T\b{XX}^T\b{W^\star} = \lambda^\star
            \end{align*}
            Therefore, we need to get the largest eigenvalue $\lambda^\star$ to maximize $\b{W}^T\b{XX}^T\b{W}$, which means $\b{W}^\star$ is the eigenvector corresponding to the largest eigenvalue. 
            \item [(ii.)]$\b{W}$ is also the solution to
            \begin{mini*}|l|
                {\b{W}}{||\b{X-WW}^T\b{X}||_F^2}{}{}\quad s.t.\quad \b{W}^T\b{W}=\b{I}_r
            \end{mini*}
            {\bf Proof:}
            \par Given that $|\b{P}|_F = \sqrt{Tr(\b{P}^T\b{P})}$, we get 
            \begin{align*}
                ||\b{X}-\b{WW}^T\b{X}||_F^2 &= Tr[(\b{X}-\b{WW}^T\b{X})^T(\b{X}-\b{WW}^T\b{X})]\\
                &= Tr[\b{X}^T\b{X}-\b{X}^T\b{WW}^T\b{X}-\b{X}^T\b{WW}^T\b{X}+\b{X}^T\b{WW}^T\b{WW}^T\b{X}]\\
                &= Tr[\b{X}^T\b{X}-\b{X}^T\b{WW}^T\b{X}]\\
                &= Tr(\b{X}^T\b{X})-Tr(\b{X}^T\b{WW}^T\b{X})\\
                &= Tr(\b{X}^T\b{X})-Tr(\b{W}^T\b{XX}^T\b{W})
            \end{align*}
            Since the first term of the equation has nothing to do with $\b{W}$, the optimization problem can be rewritten as follows:
            \begin{mini*}
                {\b{W}}{-Tr(\b{W}^T\b{XX}^T\b{W})}{}{}\quad s.t.\ \b{W}^T\b{W}=\b{I}_r
            \end{mini*}
            which is equivalent to the PCA problem.
        \end{enumerate}
        \item [(b)]
        \begin{enumerate}
            \item [(i.)] $\b{A}_2^\star$ can be solved from:
            \begin{mini*}|l|
                {\b{A}_2}{||\b{X}-\b{A}_2\b{A}_2^\dagger \b{X}||_F^2}{}{}
            \end{mini*}
            \item [(ii.)] The solution $\b{W}$ from (a) can be taken as the same as $\b{A}_2^\star$.
        \end{enumerate}
    \end{enumerate}
    \item [3.] [Ensemble Learning] Suppose there are $L$ independent two-class classifiers used for simple
    voting and the output of classifier $j\ (j = 1\cdots L)$ is denoted as $d_j$. From the point of view that the mean squared error of an estimator can be decomposed into the bias part and the variance part, explain why
    increasing $L$ can lead to an increase in classification accuracy.
    \item [4.] [Model Assessment and Selection] Suppose we carry out a $K$-fold cross-validation on a dataset
    and obtain the classification error rates $\{p_i\}_{i=1}^K$, describe the steps of a one-sided $t$ test on testing the null
    hypothesis $H_0$ that the classifier has error percentage $p_0$ or less at a significance level $\alpha$.
    

\end{enumerate}
% \includepdfmerge{HW4-Coding.pdf,1-9}
\end{document}