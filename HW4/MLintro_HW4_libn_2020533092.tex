\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{tikz}  
\usepackage{optidef}
\usepackage{pdfpages}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%  

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\rhead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{2em}

%
% Create Problem Sections
%


% \setcounter{secnumdepth}{0}
% \newcounter{partCounter}
% \newcounter{homeworkProblemCounter}
% \setcounter{homeworkProblemCounter}{1}


\newcommand{\hmwkTitle}{Homework IV}
\newcommand{\hmwkDueDate}{Dec 7th, 2022}
\newcommand{\hmwkClass}{Introduction to Machine Learing}
\newcommand{\hmwkClassInstructor}{Professor Ziping Zhao}
\newcommand{\hmwkAuthorName}{Bingnan Li}
\newcommand{\hmwkAuthorID}{2020533092}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 11:59pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}\\ \hmwkAuthorID}
\date{}

% \renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\b}[1]{\bm{#1}}
\newcommand{\PARTIAL}[2]{\frac{\partial #1}{\partial #2}}
\begin{document}

\maketitle
\pagebreak

\begin{enumerate}
	\setlength\parindent{2em}
	\item [1.] [\textit{Clustering and Mixture Models}]
	      \begin{enumerate}
		      \setlength\parindent{2em}
		      \item [(a)] K-means algorithm.\newline
		            {\bf Solution:}
		            \begin{enumerate}
                        \item Initialize K cluster centers $m_i$ by randomly selecting K input data points.
                        \item Repeat the following procedure until convergence:
                        \begin{enumerate}
                            \item For all $x^{(l)}\in\mathcal{X}$, we obtain the estimated labels
                            \begin{align*}
                                b_i^{(l)} = \left\{\begin{aligned}
                                    1,\ &if\ i=\arg\min_j||x^{(l)}-m_j||\\
                                    0,\ &elsewhere
                                \end{aligned}\right.
                            \end{align*}
                        \item For all $m_i$, we obtain 
                        \[m_i=\frac{\sum_lb_i^{(l)}x^{(l)}}{\sum_lb_i^{(l)}}\]
                        \end{enumerate}
                    \end{enumerate}
		      \item [(b)] Cluster the samples into 2 clusters.\newline
		      {\bf Solution:}
              \par First, we select $m_1 = (0,0)$ and $m_2=(5,0)$ as initialized cluster center. Then for the first iteration, we have the following result:
              \begin{align*}
                b_1^{(1)}&=1\quad b_2^{(1)}=0\\
                b_1^{(2)}&=1\quad b_2^{(2)}=0\\
                b_1^{(3)}&=1\quad b_2^{(3)}=0\\
                b_1^{(4)}&=0\quad b_2^{(4)}=1\\
                b_1^{(5)}&=0\quad b_2^{(5)}=1\\
                m_1&=\frac{(0,2)+(0,0)+(1,0)}{3}=(\frac{1}{3},\frac{2}{3})\\
                m_2 &= \frac{(5,0)+(5,2)}{2}=(5,1)
              \end{align*}
              Next, for the second iteration, we find that 
              \begin{align*}
                b_1^{(1)}&=1\quad b_2^{(1)}=0\\
                b_1^{(2)}&=1\quad b_2^{(2)}=0\\
                b_1^{(3)}&=1\quad b_2^{(3)}=0\\
                b_1^{(4)}&=0\quad b_2^{(4)}=1\\
                b_1^{(5)}&=0\quad b_2^{(5)}=1\\
                m_1&=\frac{(0,2)+(0,0)+(1,0)}{3}=(\frac{1}{3},\frac{2}{3})\\
                m_2 &= \frac{(5,0)+(5,2)}{2}=(5,1)
              \end{align*}
              The result converged, so we terminated the algorithm and cluster centers are 
              \[m_1=(\frac{1}{3},\frac{2}{3})\quad m_2 = (5,1)\]
	      \end{enumerate}
	\item [2.] [\textit{Clustering and Mixture Models}]
	      \begin{enumerate}
		      \setlength\parindent{2em}
		      \item Advantages of GMM and Why it can be used for clustering.\newline
		      {\bf Solution:}
              \par Advantages: GMM is a kind of "soft-label" method, the projected data do not represent deterministic classification label but the probability of belonging to any classes.
              \par Why it can be used for clustering: K-means is a special case of GMM. In practice, the higher the $h_i^{(l)}$ is, the more likely that $x^{(l)}$ is generated by component $\mathcal{G}_i$, which can be interpreted as $x^{(l)}$ belongs to cluster $i$.
		      \item Estimate the parameters of the GMM.
		      {\bf Solution:}
              \par By definition, we have 
              \begin{align*}
                h_i^{(l)} &= \frac{P(x^{(l)}|\mathcal{G}_i,\b{\phi}^t)\pi_i}{\sum_jP(x^{(l)}|\mathcal{G}_j,\b{\phi}^t)\pi_j}\\
                &= \frac{|\Sigma_i|^{-\frac{1}{2}}\exp{[-\frac{1}{2}(\b{x_l}-\b{\mu_i})^T(\Sigma)^{-1}(\b{x_l}-\b{\mu_i})]\pi_i}}{\sum_{j=1}^K|\Sigma_j|^{-\frac{1}{2}}\exp{[-\frac{1}{2}(\b{x_l}-\b{\mu_j})^T(\Sigma)^{-1}(\b{x_l}-\b{\mu_j})]\pi_j}}\\
                &= \frac{\mathcal{N}(\b{x}_l|\b{\mu_i,\Sigma_i})\pi_i}{\sum_{j=1}^K\mathcal{N}(\b{x}_l|\b{\mu_j,\Sigma_j})\pi_j}
              \end{align*}
              and 
              \begin{align*}
                \mathcal{Q}(\b{\phi}|\b{\phi}^t)=\sum_l\sum_ih_i^{(l)}[\log{\pi_i}+\log{\mathcal{N}(\b{x}_l|\b{\mu_i,\Sigma_i})}]
              \end{align*}
              Then, maximization of $\mathcal{Q}(\b{\phi}|\b{\phi}^t)$ is equivalent to 
              \begin{maxi*}|l|
                {\{\pi_i\},\{\b{\mu}_i\},\{\Sigma_i\}}{\mathcal{Q}(\b{\phi}|\b{\phi}^t)=\sum_l\sum_ih_i^{(l)}\log{\pi_i}+h_i^{(l)}\log{\mathcal{N}(\b{x}_l|\b{\mu_i,\Sigma_i})}}{}{}
                \addConstraint{\sum_i\pi_i=1}{}
            \end{maxi*}
            Since the second term does not depend on $\pi_i$, the problem for $\{\pi_i\}$ is 
            \begin{maxi*}|l|
                {\{\pi_i\}}{\sum_l\sum_ih_i^{(l)}\log{\pi_i}}{}{}
                \addConstraint{\sum_i\pi_i=1}{}
            \end{maxi*}
            By using Lagrangian, we solve for 
            \[\PARTIAL{}{\pi_i}\left[\sum_l\sum_ih_i^{(l)}\log{\pi_i}-\lambda\left(\sum_i\pi_i-1\right)\right]=0\]
            And we get 
            \[\pi_i=\frac{\sum_lh_i^{(l)}}{N}\]
	      \end{enumerate}
	\item [3.] [\textit{Nonparametric Density Estimation}]
	      \begin{enumerate}
		      \setlength\parindent{2em}
		      \item Expression of $\hat{p}(x)$.
		      \item Expression of $L'(h)$ based on the histogram estimator $\hat{p}(x)$.
		      \item $h$ that minimizes $L'(h)$.
	      \end{enumerate}
	\item [4.] [Nonparametric Regression]
	      \begin{enumerate}
		      \setlength\parindent{2em}
		      \item Estimated output $\hat{y}$ and is linear regression a linear smoother?
		      \item In kernel regression, if we use kernel $K(x_i,x)=exp\left\{\frac{-||x_i-x||^2}{2\sigma^2}\right\}$, given an input $x$, please derive the estimated output $\hat{y}$. Furthermore, is this kernel regression a linear smoother?
	      \end{enumerate}
\end{enumerate}
% \includepdfmerge{HW3-Coding.pdf,1-5}
\end{document}