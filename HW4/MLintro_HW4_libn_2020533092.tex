\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{tikz}  
\usepackage{optidef}
\usepackage{pdfpages}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%  

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\rhead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{2em}

%
% Create Problem Sections
%


% \setcounter{secnumdepth}{0}
% \newcounter{partCounter}
% \newcounter{homeworkProblemCounter}
% \setcounter{homeworkProblemCounter}{1}


\newcommand{\hmwkTitle}{Homework IV}
\newcommand{\hmwkDueDate}{Dec 7th, 2022}
\newcommand{\hmwkClass}{Introduction to Machine Learing}
\newcommand{\hmwkClassInstructor}{Professor Ziping Zhao}
\newcommand{\hmwkAuthorName}{Bingnan Li}
\newcommand{\hmwkAuthorID}{2020533092}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 11:59pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}\\ \hmwkAuthorID}
\date{}

% \renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\renewcommand{\b}[1]{\bm{#1}}
\newcommand{\PARTIAL}[2]{\frac{\partial #1}{\partial #2}}
\begin{document}

\maketitle
\pagebreak

\begin{enumerate}
	\setlength\parindent{2em}
	\item [1.] [\textit{Clustering and Mixture Models}]
	      \begin{enumerate}
		      \setlength\parindent{2em}
		      \item [(a)] K-means algorithm.\newline
		            {\bf Solution:}
		            \begin{enumerate}
                        \item Initialize K cluster centers $m_i$ by randomly selecting K input data points.
                        \item Repeat the following procedure until convergence:
                        \begin{enumerate}
                            \item For all $x^{(l)}\in\mathcal{X}$, we obtain the estimated labels
                            \begin{align*}
                                b_i^{(l)} = \left\{\begin{aligned}
                                    1,\ &if\ i=\arg\min_j||x^{(l)}-m_j||\\
                                    0,\ &elsewhere
                                \end{aligned}\right.
                            \end{align*}
                        \item For all $m_i$, we obtain 
                        \[m_i=\frac{\sum_lb_i^{(l)}x^{(l)}}{\sum_lb_i^{(l)}}\]
                        \end{enumerate}
                    \end{enumerate}
		      \item [(b)] Cluster the samples into 2 clusters.\newline
		      {\bf Solution:}
              \par First, we select $m_1 = (0,0)$ and $m_2=(5,0)$ as initialized cluster center. Then for the first iteration, we have the following result:
              \begin{align*}
                b_1^{(1)}&=1\quad b_2^{(1)}=0\\
                b_1^{(2)}&=1\quad b_2^{(2)}=0\\
                b_1^{(3)}&=1\quad b_2^{(3)}=0\\
                b_1^{(4)}&=0\quad b_2^{(4)}=1\\
                b_1^{(5)}&=0\quad b_2^{(5)}=1\\
                m_1&=\frac{(0,2)+(0,0)+(1,0)}{3}=(\frac{1}{3},\frac{2}{3})\\
                m_2 &= \frac{(5,0)+(5,2)}{2}=(5,1)
              \end{align*}
              Next, for the second iteration, we find that 
              \begin{align*}
                b_1^{(1)}&=1\quad b_2^{(1)}=0\\
                b_1^{(2)}&=1\quad b_2^{(2)}=0\\
                b_1^{(3)}&=1\quad b_2^{(3)}=0\\
                b_1^{(4)}&=0\quad b_2^{(4)}=1\\
                b_1^{(5)}&=0\quad b_2^{(5)}=1\\
                m_1&=\frac{(0,2)+(0,0)+(1,0)}{3}=(\frac{1}{3},\frac{2}{3})\\
                m_2 &= \frac{(5,0)+(5,2)}{2}=(5,1)
              \end{align*}
              The result converged, so we terminated the algorithm and cluster centers are 
              \[m_1=(\frac{1}{3},\frac{2}{3})\quad m_2 = (5,1)\]
	      \end{enumerate}
	\item [2.] [\textit{Clustering and Mixture Models}]
	      \begin{enumerate}
		      \setlength\parindent{2em}
		      \item Advantages of GMM and Why it can be used for clustering.\newline
		      {\bf Solution:}
              \par Advantages: GMM is a kind of "soft-label" method, the projected data do not represent deterministic classification label but the probability of belonging to any classes.
              \par Why it can be used for clustering: K-means is a special case of GMM. In practice, the higher the $h_i^{(l)}$ is, the more likely that $x^{(l)}$ is generated by component $\mathcal{G}_i$, which can be interpreted as $x^{(l)}$ belongs to cluster $i$.
		      \item Estimate the parameters of the GMM.\newline
		      {\bf Solution:}
              \par Define $\mathcal{Q}(\b{\phi}|\b{\phi^t})$ as following
              \begin{align*}
                \mathcal{Q}(\b{\phi}|\b{\phi^t}) &= \E[\mathcal{L}_C(\b{\phi}|\mathcal{X},\mathcal{Z})|\mathcal{X},\b{\phi^t}]\\
                where&\\
                \mathcal{L}_C(\b{\phi}) &= \log\prod_{l}p(x^{(l)},z^{(l)}|\b{\phi})\\
                &= \sum_l\left[\log P(\b{z}^{(l)}|\b{\phi})+\log p(\b{x}^{(l)}|\b{z}^{(l)},\b{\phi})\right]\\
                &= \sum_l\sum_i z_i^{(l)}[\log \pi_i + \log p_i(\b{x}^{(l)|\b{\phi}})]
              \end{align*}
              Hence
              \begin{align*}
                \mathcal{Q}(\b{\phi}|\b{\phi^t}) &= \E[\mathcal{L}_C(\b{\phi}|\mathcal{X},\mathcal{Z})|\mathcal{X},\b{\phi^t}]\\
                &= \sum_l\sum_i \E[z_i^{(l)}|\mathcal{X},\b{\phi^t}][\log \pi_i + \log p_i(\b{x}^{(l)|\b{\phi}})]
              \end{align*}
              where
              \begin{align*}
                \E[z_i^{(l)}|\mathcal{X},\b{\phi^t}] &= \E[z_i^{(l)}|\b{x}^{(l)},\b{\phi}]\\
                &= P(z_i^{(l)}=1|\b{x}^{(l)},\b{\phi^t})\\
                &= \frac{p(\b{x}^{(l)}|z_i^{(l)}=1,\b{\phi^t})P(z_i^{(l)}=1|\b{\phi^t})}{p(\b{x}^{(l)}|\b{\phi^t})}\\
                &= \frac{p_i(\b{x}^{(l)}|\b{\phi^t})\pi_i}{\sum_jp_j(\b{x}^{(l)}|\b{\phi^t})\pi_j}\\
                &= \frac{P(x^{(l)}|\mathcal{G}_i,\b{\phi}^t)\pi_i}{\sum_jP(x^{(l)}|\mathcal{G}_j,\b{\phi}^t)\pi_j}\\
                &\equiv h_i^{(l)}
              \end{align*}
              Therefore, we have 
              \begin{align*}
                h_i^{(l)} &= \frac{P(x^{(l)}|\mathcal{G}_i,\b{\phi}^t)\pi_i}{\sum_jP(x^{(l)}|\mathcal{G}_j,\b{\phi}^t)\pi_j}\\
                &= \frac{|\Sigma_i|^{-\frac{1}{2}}\exp{[-\frac{1}{2}(\b{x_l}-\b{\mu_i})^T(\Sigma_i)^{-1}(\b{x_l}-\b{\mu_i})]\pi_i}}{\sum_{j=1}^K|\Sigma_j|^{-\frac{1}{2}}\exp{[-\frac{1}{2}(\b{x_l}-\b{\mu_j})^T(\Sigma_j)^{-1}(\b{x_l}-\b{\mu_j})]\pi_j}}\\
                &= \frac{\mathcal{N}(\b{x}_l|\b{\mu_i,\Sigma_i})\pi_i}{\sum_{j=1}^K\mathcal{N}(\b{x}_l|\b{\mu_j,\Sigma_j})\pi_j}
              \end{align*}
              and 
              \begin{align*}
                \mathcal{Q}(\b{\phi}|\b{\phi}^t)=\sum_l\sum_ih_i^{(l)}[\log{\pi_i}+\log{\mathcal{N}(\b{x}_l|\b{\mu_i,\Sigma_i})}]
              \end{align*}
              Then, maximization of $\mathcal{Q}(\b{\phi}|\b{\phi}^t)$ is equivalent to 
              \begin{maxi*}|l|
                {\{\pi_i\},\{\b{\mu}_i\},\{\Sigma_i\}}{\mathcal{Q}(\b{\phi}|\b{\phi}^t)=\sum_l\sum_ih_i^{(l)}\log{\pi_i}+h_i^{(l)}\log{\mathcal{N}(\b{x}_l|\b{\mu_i,\Sigma_i})}}{}{}
                \addConstraint{\sum_i\pi_i=1}{}
            \end{maxi*}
            Since the second term does not depend on $\pi_i$, the problem for $\{\pi_i\}$ is 
            \begin{maxi*}|l|
                {\{\pi_i\}}{\sum_l\sum_ih_i^{(l)}\log{\pi_i}}{}{}
                \addConstraint{\sum_i\pi_i=1}{}
            \end{maxi*}
            By using Lagrangian, we solve for 
            \[\PARTIAL{}{\pi_i}\left[\sum_l\sum_ih_i^{(l)}\log{\pi_i}-\lambda\left(\sum_i\pi_i-1\right)\right]=0\]
            And we get 
            \[\pi_i=\frac{\sum_lh_i^{(l)}}{N}\]
            Then the first term of $\mathcal{Q}$ does not depend on $\b{\mu_i},\Sigma_i$. Hence, the problem for $\b{\mu_i},\Sigma_i$ is 
            \begin{maxi*}|l|
                {\{\b{\mu}_i\},\{\Sigma_i\}}{\sum_l\sum_ih_i^{(l)}\log{\mathcal{N}(\b{x}_l|\b{\mu_i,\Sigma_i})}}{}{}
            \end{maxi*}
            By solving 
            \[\PARTIAL{}{\mu_i}\left[\sum_l\sum_ih_i^{(l)}\log{\mathcal{N}(\b{x}_l|\b{\mu_i,\Sigma_i})}\right]=0\]
            and 
            \[\PARTIAL{}{\Sigma_i}\left[\sum_l\sum_ih_i^{(l)}\log{\mathcal{N}(\b{x}_l|\b{\mu_i,\Sigma_i})}\right]=0\]
            we get 
            \[\b{\mu}_i^{t+1}=\frac{\sum_lh_i^{(l)}\b{x}_l}{\sum_lh_i^{(l)}}\]
            and 
            \[\b{\Sigma}_i^{t+1}=\frac{\sum_lh_i^{(l)}(\b{x}_l-\b{\mu}_i^{t+1})(\b{x}_l-\b{\mu}_i^{t+1})^T}{\sum_lh_i^{(l)}}\]
	      \end{enumerate}
	\item [3.] [\textit{Nonparametric Density Estimation}]
	      \begin{enumerate}
		      \setlength\parindent{2em}
		      \item Expression of $\hat{p}(x)$.\newline
		      {\bf Proof:}
          \par By definition, the histogram estimator is defined as following:
          \[\hat{p}(x)=\frac{\#\{x^{(l)}\ in\ the\ same\ bin\ as\ x\}}{nh}=\frac{\#\left\{x^{(l)}\in\left[\left\lfloor{\frac{x}{h}}\right\rfloor h,\left\lceil{\frac{x}{h}}\right\rceil h\right)\right\}}{nh}\]
		      \item Expression of $L'(h)$ based on the histogram estimator $\hat{p}(x)$.\newline
		      {\bf Proof:}
		      \par First, for the first term of $L'$, we can split the integral by bins:
          \begin{align*}
            \int_0^1\hat{p}^2(x)dx &= \sum_{j=0}^{\lfloor\frac{1}{h}\rfloor}\int_{jh}^{(j+1)h}\frac{\#^2\left\{x^{(l)}\in\left[jh, (j+1)h\right)\right\}}{n^2h^2}dx\\
            &= \frac{\sum_{j=0}^{\lfloor\frac{1}{h}\rfloor}\#^2\left\{x^{(l)}\in\left[jh, (j+1)h\right)\right\}}{n^2h}
          \end{align*}
          For the second term of $L'$, we can rewrite it as following:
          \begin{align*}
            \frac{2}{n}\sum_{i=1}^n\hat{p}(x_i) &= \frac{2}{n}\sum_{i=1}^n\frac{\#\left\{x^{(l)}\in\left[\left\lfloor{\frac{x}{h}}\right\rfloor h,\left\lceil{\frac{x}{h}}\right\rceil h\right)\right\}}{nh}\\
            &= \frac{2\sum_{j=0}^{\lfloor\frac{1}{h}\rfloor}\#^2\left\{x^{(l)}\in\left[jh, (j+1)h\right)\right\}}{n^2h}
          \end{align*}
          Hence, we have
          \[L'(h)=\int_0^1\hat{p}^2(x)dx-\frac{2}{n}\sum_{i=1}^n\hat{p}(x_i)=-\frac{\sum_{j=0}^{\lfloor\frac{1}{h}\rfloor}\#^2\left\{x^{(l)}\in\left[jh, (j+1)h\right)\right\}}{n^2h}\]
		      \item $h$ that minimizes $L'(h)$.\newline
		      {\bf Proof:}
          \par Since as h goes to 0, as long as there are not coincide sample points, the numerator of $L'(h)$ will go to $n$. However, the denominator of $L'(h)$ is $n^2h$, which goes to 0 as $h\to 0$, thus $\lim_{h\to 0}L'(h) = -\infty$.
          \[h = \arg minimize(L'(h)) = 0\]
          % \par Since $\mathcal{X}$ are drawn from $p(x)$, then by the law of large number, we know that as the sample size goes into infinity, we have 
          % \begin{align*}
          %   L'(h) &=-\frac{\sum_{j=0}^{\lfloor\frac{1}{h}\rfloor}\#^2\left\{x^{(l)}\in\left[jh, (j+1)h\right)\right\}}{n^2h}\\
          %   &= -\frac{\sum_{i=1}^{\lfloor\frac{1}{h}\rfloor}\left[\int_{jh}^{(j+1)h}p(x)dx\cdot n\right]^2}{n^2h}\\
          %   &= -\frac{\sum_{i=1}^{\lfloor\frac{1}{h}\rfloor}\left[\int_{jh}^{(j+1)h}p(x)dx\right]^2}{h}
          % \end{align*}
          % By solving the differentiation of $L'(h)$, we have 
          % \begin{align*}
          %   \frac{dL'(h)}{h} &= \frac{\sum_{i=1}^{\lfloor\frac{1}{h}\rfloor}\left[\int_{jh}^{(j+1)h}p(x)dx\right]\left[\int_{jh}^{(j+1)h}p(x)dx-2\left[p((j+1)h)-p(jh)\right]h\right]}{h^2}
          % \end{align*}
          % Consider the numerator, we know that $\int_{jh}^{(j+1)h}p(x)dx$ is always positive or zero. Then for the second part, by the {\it Mean Value Theorem}, we know that there exit a real number $\xi_j\in[jh, (j+1)h)$, such that $\int_{jh}^{(j+1)h}p(x)dx = p(\xi_j)h$. Based on that, we have 
          % \begin{align*}
          %   &\int_{jh}^{(j+1)h}p(x)dx-2\left[p((j+1)h)-p(jh)\right]h\\
          %   &= p(\xi_j)h-2\left[p((j+1)h)-p(jh)\right]h\\
          %   &= h\left\{p(\xi_j)-2\left[p((j+1)h-p(jh))\right]\right\}
          % \end{align*}
          % Assume that $p(x)$ is continuous in $[0,1]$, we have 
	      \end{enumerate}
	\item [4.] [Nonparametric Regression]
	      \begin{enumerate}
		      \setlength\parindent{2em}
		      \item Estimated output $\hat{y}$ and is linear regression a linear smoother?\newline
		      {\bf Solution:}
          \par Given that the least squares estimate for $\b{w}$ is 
          \[\b{w}^*=\left(H^TH\right)^{-1}H^TY\]
          we have the following estimated output $\hat{y}$
          \begin{align*}
            \hat{y} &= (\b{w}^*)^T\cdot\b{h(x)}\\
            &= \left[\left(H^TH\right)^{-1}H^TY\right]^T\b{h(x)}\\
            &= Y^TH\left(H^TH\right)^{-1}\b{h(x)}\\
            &= \left[H\left(H^TH\right)^{-1}\b{h(x)}\right]^TY\\
            &= \b{h(x)}^T\left(H^TH\right)^{-1}H^TY\\
            &\Rightarrow\\
            \b{l(x)} &= H\left(H^TH\right)^{-1}\b{h(x)}
          \end{align*}
          Hence, linear regression is a linear smoother.
		      \item In kernel regression, if we use kernel $K(x_i,x)=exp\left\{\frac{-||x_i-x||^2}{2\sigma^2}\right\}$, given an input $x$, please derive the estimated output $\hat{y}$. Furthermore, is this kernel regression a linear smoother?\newline
		      {\bf Solution:}
          \par By the definition of Kernel regression, we have the estimated output $\hat{y}$ as following:
          \begin{align*}
            \hat{y} &= \frac{\sum_{i=1}^nK(x_i,x)y_i}{\sum_{i=1}^nK(x_i,x)}\\
            &= \frac{\sum_{i=1}^n \exp{\left\{-\frac{||x_i-x||^2}{2\sigma^2}\right\}}y_i}{\sum_{i=1}^n \exp{\left\{-\frac{||x_i-x||^2}{2\sigma^2}\right\}}}\\
            &= \sum_{i=1}^n softmax\left(\frac{||x_i-x||^2}{2\sigma^2}\right)y_i
          \end{align*}
          Then define that 
          \begin{align*}
            S = \begin{bmatrix}
              softmax\left(\frac{||x_1-x||^2}{2\sigma^2}\right)\\
              softmax\left(\frac{||x_2-x||^2}{2\sigma^2}\right)\\
              \vdots\\
              softmax\left(\frac{||x_n-x||^2}{2\sigma^2}\right)
            \end{bmatrix}
          \end{align*}
          we have
          \begin{align*}
            \hat{y} &= \sum_{i=1}^n softmax\left(\frac{||x_i-x||^2}{2\sigma^2}\right)y_i\\
            &= S^TY\\
            &\Rightarrow\\
            \b{l(x)} &= S = \begin{bmatrix}
              softmax\left(\frac{||x_1-x||^2}{2\sigma^2}\right)\\
              softmax\left(\frac{||x_2-x||^2}{2\sigma^2}\right)\\
              \vdots\\
              softmax\left(\frac{||x_n-x||^2}{2\sigma^2}\right)
            \end{bmatrix}
          \end{align*}
          Hence, this kernel regression is a linear smoother.
	      \end{enumerate}
\end{enumerate}
\includepdfmerge{HW4-Coding.pdf,1-9}
\end{document}