{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020d28ac",
   "metadata": {},
   "source": [
    "# Fit GMM by EM algorithm\n",
    "\n",
    "*Please convert your coding notebook from .ipynb into <span style=\"color:red\">.pdf</span> and concatenate it with your writing part.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ffcc8",
   "metadata": {},
   "source": [
    "You need to implement the EM algorithm, using closed-forms derived in './HW4-Writing.pdf/2(b)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca107a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT change this cell.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-step: compute posterior probabilities h_{li}\n",
    "def probabilities(data, weights, means, covariances):\n",
    "    '''\n",
    "    :param data: size: (N, D)\n",
    "    :param weights: corresponds to \\pi, size: (K,)\n",
    "    :param means: corresponds to \\mu, size: (K, D)\n",
    "    :param covariances: corresponds to \\sigma, size: (K, D, D)\n",
    "    :return prob_matrix: a matrix filled by posterior probability h_{ik}, size (N, K)\n",
    "    '''\n",
    "    num_data = len(data)\n",
    "    num_clusters = len(means)\n",
    "    prob_matrix = np.zeros((num_data, num_clusters))\n",
    "    ########## START YOUR CODE HERE ##########\n",
    "    \n",
    "     pass\n",
    "    \n",
    "    ########## END YOUR CODE HERE ##########\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M-step: update weights \\pi, means \\mu, and covariances \\sigma\n",
    "def updates(data, prob_matrix, weights, means, covariances):\n",
    "    num_clusters = len(means)\n",
    "    num_data = len(data)\n",
    "    dim = data.shape[1]\n",
    "    ########## START YOUR CODE HERE ##########\n",
    "    \n",
    "    pass\n",
    "\n",
    "    ########## END YOUR CODE HERE ##########\n",
    "    return weights, means, covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06985570",
   "metadata": {},
   "source": [
    "To help us develop and test our implementation, we will generate some observations from a mixture of Gaussians and then run our EM algorithm to discover the mixture components. We'll begin with a function to generate the data, and a quick plot to visualize its output for a 2-dimensional mixture of three Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "\n",
    "def generate_MoG_data(num_data, means, covariances, weights):\n",
    "    \"\"\" Creates a list of data points \"\"\"\n",
    "    num_clusters = len(weights)\n",
    "    data = []\n",
    "    for i in range(num_data):\n",
    "        #  Use np.random.choice and weights to pick a cluster id greater than\n",
    "        #  or equal to 0 and less than num_clusters.\n",
    "        k = np.random.choice(len(weights), 1, p=weights)[0]\n",
    "        # Use np.random.multivariate_normal to create data from this cluster\n",
    "        x = np.random.multivariate_normal(means[k], covariances[k])\n",
    "        data.append(x)\n",
    "    return data\n",
    "\n",
    "# Model parameters\n",
    "data_means = np.array([\n",
    "    [5, 0], # mean of cluster 1\n",
    "    [1, 1], # mean of cluster 2\n",
    "    [0, 5]  # mean of cluster 3\n",
    "])\n",
    "data_covariances = np.array([\n",
    "    [[.5, 0.], [0, .5]], # covariance of cluster 1\n",
    "    [[.92, .38], [.38, .91]], # covariance of cluster 2\n",
    "    [[.5, 0.], [0, .5]]  # covariance of cluster 3\n",
    "])\n",
    "data_weights = np.array([1/4., 1/2., 1/4.])  # weights of each cluster\n",
    "\n",
    "np.random.seed(4)\n",
    "data = np.array(generate_MoG_data(100, data_means, data_covariances, data_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f03cf3",
   "metadata": {},
   "source": [
    "Now plot the data you created above. The plot should be a scatterplot with 100 points that appear to roughly fall into three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "d = np.vstack(data)\n",
    "plt.plot(d[:,0], d[:,1],'ko')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81b9b6",
   "metadata": {},
   "source": [
    "Now we'll fit a mixture of Gaussians to this data using our implementation of the EM algorithm. As with k-means, it is important to ask how we obtain an initial configuration of mixing weights and component parameters. In this simple case, we'll take three random points to be the initial cluster means, use the empirical covariance of the data to be the initial covariance in each cluster (a clear overestimate), and set the initial mixing weights to be uniform across clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd77702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of parameters\n",
    "np.random.seed(4)\n",
    "chosen = np.random.choice(len(data), 3, replace=False)\n",
    "initial_means = np.array([data[x] for x in chosen])\n",
    "initial_covariances = np.array([np.cov(data, rowvar=0)] * 3)\n",
    "initial_weights = np.array([1/3., 1/3., 1/3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d782be",
   "metadata": {},
   "source": [
    "We will use the following plot_contours() function to visualize the Gaussian components over the data at three different points in the algorithm's execution:\n",
    "\n",
    "1. At initialization (using initial_means, initial_covariances, and initial_weights)\n",
    "2. After running the algorithm to completion\n",
    "3. After 20 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa4be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bivariate_normal(X, Y, sigmax=1.0, sigmay=1.0,\n",
    "                 mux=0.0, muy=0.0, sigmaxy=0.0):\n",
    "    Xmu = X-mux\n",
    "    Ymu = Y-muy\n",
    "    rho = sigmaxy/(sigmax*sigmay)\n",
    "    z = Xmu**2/sigmax**2 + Ymu**2/sigmay**2 - 2*rho*Xmu*Ymu/(sigmax*sigmay)\n",
    "    denom = 2*np.pi*sigmax*sigmay*np.sqrt(1-rho**2)\n",
    "    return np.exp(-z/(2*(1-rho**2))) / denom\n",
    "\n",
    "def plot_contours(data, means, covs, title):\n",
    "    plt.figure()\n",
    "    plt.plot([x[0] for x in data], [y[1] for y in data],'ko') # data\n",
    "\n",
    "    delta = 0.025\n",
    "    k = len(means)\n",
    "    x = np.arange(-2.0, 7.0, delta)\n",
    "    y = np.arange(-2.0, 7.0, delta)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    col = ['green', 'red', 'indigo']\n",
    "    for i in range(k):\n",
    "        mean = means[i]\n",
    "        cov = covs[i]\n",
    "        sigmax = np.sqrt(cov[0][0])\n",
    "        sigmay = np.sqrt(cov[1][1])\n",
    "        sigmaxy = cov[0][1]/(sigmax*sigmay)\n",
    "        Z = bivariate_normal(X, Y, sigmax, sigmay, mean[0], mean[1], sigmaxy)\n",
    "        plt.contour(X, Y, Z, colors = col[i])\n",
    "        plt.title(title)\n",
    "    plt.rcParams.update({'font.size':16})\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c875d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters after initialization\n",
    "plot_contours(data, initial_means, initial_covariances, 'Initial clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec67c72",
   "metadata": {},
   "source": [
    "Now run the EM algorithm and plot contours afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219da7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EM\n",
    "my_means = initial_means\n",
    "my_weights = initial_weights\n",
    "my_covariances = initial_covariances\n",
    "iters = 20\n",
    "for iter in range(iters):\n",
    "    # Here we run 20 iterations. You can use a more strict convergence criterion,\n",
    "    # such as that the increment of the log-likelihood function is less than 1e-3\n",
    "    prob_matrix = probabilities(data, my_weights, my_means, my_covariances)\n",
    "    [my_weights, my_means, my_covariances] = updates(data, prob_matrix, my_weights, my_means, my_covariances)\n",
    "print(\"my_weights =\\n{},\\n\\nmy_means =\\n{},\\n\\nmy_covariances =\\n{}\".format(my_weights, my_means, my_covariances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db83a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters after running EM to convergence\n",
    "plot_contours(data, my_means, my_covariances, 'Final clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e89b7",
   "metadata": {},
   "source": [
    "Now call the GaussianMixture() method in sklearn package, and see its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm_sklearn = GaussianMixture(n_components=3, covariance_type='full',\n",
    "                              max_iter=20, weights_init=initial_weights,\n",
    "                              means_init=initial_means).fit(data)\n",
    "print(\"weights_sklearn =\\n{},\\n\\nmeans_sklearn =\\n{},\\n\\ncovariance_sklearn =\\n{}\".format(gmm_sklearn.weights_, gmm_sklearn.means_, gmm_sklearn.covariances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters after running EM to convergence\n",
    "plot_contours(data, gmm_sklearn.means_, gmm_sklearn.covariances_, 'Clusters by sklearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920f673",
   "metadata": {},
   "source": [
    "Now see the differences of parameters of your algorithm and that of sklearn. You should see the precision is at least **1e-3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After 20 iterations, differences between our algorithm and sklearn:\\n\")\n",
    "print(\"delta_weights =\\n{},\\n\\ndelta_means =\\n{},\\n\\ndelta_covariances=\\n{}\".format(my_weights-gmm_sklearn.weights_, my_means-gmm_sklearn.means_, my_covariances-gmm_sklearn.covariances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2d307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb5f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "æ— ",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
